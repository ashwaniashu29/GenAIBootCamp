{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_jbFcxFZhG5K",
        "outputId": "74e4fef3-3519-48c6-f844-31336d41970c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: keras-nlp in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Requirement already satisfied: keras-preprocessing in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: tensorflow-text==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: np_utils in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.15.0) (0.16.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.15.0) (2.15.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.1.7)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (24.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (2023.12.25)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (13.7.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.1.8)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras-preprocessing) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub>=0.13.0->tensorflow-text==2.15.0) (2.15.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-nlp) (2.31.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-nlp) (0.0.8)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-nlp) (2.16.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras-nlp) (2024.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text==2.15.0) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob 'keras-nlp' 'keras-preprocessing' 'tensorflow-text==2.15.0' np_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iklSJ4lqUQlT",
        "outputId": "0cf03f35-350d-44c8-fe6a-d57338e0a486",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from textblob import TextBlob, Word\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import warnings\n",
        "import nltk\n",
        "\n",
        "TRACE = False  # Setting to true is useful when debugging to know which device is being used\n",
        "embedding_dim = 50\n",
        "epochs=2\n",
        "batch_size = 64\n",
        "BATCH = True\n",
        "\n",
        "def set_seeds_and_trace():\n",
        "  os.environ['PYTHONHASHSEED'] = '0'\n",
        "  np.random.seed(42)\n",
        "  tf.random.set_seed(42)\n",
        "  random.seed(42)\n",
        "  if TRACE:\n",
        "    tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "def set_session_with_gpus_and_cores():\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "  config = tf.compat.v1.ConfigProto( device_count = {'GPU': gpus  , 'CPU': cores} , intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  sess = tf.compat.v1.Session(config=config)\n",
        "  tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "set_seeds_and_trace()\n",
        "set_session_with_gpus_and_cores()\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('punkt')\n",
        "textblob_tokenizer = lambda x: TextBlob(x).words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l13de14sclyD",
        "outputId": "fbdc1797-7447-44cf-a7a7-16292a5e0067",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting get_data.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile get_data.sh\n",
        "if [ ! -f yelp.csv ]; then\n",
        "  wget -O yelp.csv https://www.dropbox.com/s/xds4lua69b7okw8/yelp.csv?dl=0\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PvRXU9EMVJMp"
      },
      "outputs": [],
      "source": [
        "!bash get_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QAWXcLEieD4E"
      },
      "outputs": [],
      "source": [
        "path = './yelp.csv'\n",
        "yelp = pd.read_csv(path)\n",
        "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
        "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
        "X = yelp_best_worst.text\n",
        "y = yelp_best_worst.stars.map({1:0, 5:1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ljgSnKkzeM4-"
      },
      "outputs": [],
      "source": [
        "# Create corpus of sentences such that the sentence has more than 3 words\n",
        "corpus = [sentence for sentence in X if len(textblob_tokenizer(sentence))>3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-AyyCRQ2-7J",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "At this point we have a list (any iterable will do) of queries that are longer than 3 words. This is normal to filter random queries. Now we must use the `Tokenizer` object to `fit` on the corpus, in order to convert each wor to an ID, and later convert such corpus of list of words into their identifiers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dUlTe1xsgi51",
        "outputId": "b2caa57a-a97f-49af-8f65-bae07defac5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before the tokenizer: ['My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\n\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I\\'ve ever had.  I\\'m pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\n\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I\\'ve ever had.\\n\\nAnyway, I can\\'t wait to go back!']\n",
            "After the tokenizer: [[12, 447, 202, 35, 41, 20, 12, 571, 11, 282, 2, 9, 8, 196, 1, 1549, 8, 201, 71, 123, 654, 319, 4500, 43, 2394, 58, 1408, 1478, 50, 483, 8, 196, 2, 50, 28, 572, 664, 20, 1, 3444, 458, 616, 450, 9, 388, 38, 1, 27, 4501, 53, 178, 664, 25, 1, 1631, 15, 46, 41, 1, 138, 85, 600, 4, 1632, 2, 46, 43, 2217, 2726, 9, 8, 1388, 2, 693, 1, 66, 74, 109, 23, 86, 178, 163, 17, 77, 356, 632, 45, 43, 1036, 2, 2395, 80, 130, 54, 15, 113, 9, 9, 8, 99, 170, 140, 20, 1, 122, 545, 196, 3, 23, 1, 475, 2218, 3230, 770, 1409, 2727, 2, 9, 8, 301, 2, 108, 9, 154, 16, 144, 859, 6, 43, 8190, 243, 16, 8, 99, 2, 9, 364, 123, 1, 179, 998, 9, 8, 1, 66, 812, 74, 109, 23, 750, 3, 142, 139, 5, 48, 64]]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "# Use the fit_on_texts method to fit the tokenizer\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "# Fill\n",
        "\n",
        "print(f'Before the tokenizer: {corpus[:1]}')\n",
        "\n",
        "#Now use the same \"trained\" tokenizer to convert the corpus from words to IDs with the texts_to_sequences method\n",
        "tokenized_corpus = tokenizer.texts_to_sequences(corpus)\n",
        "\n",
        "print(f'After the tokenizer: {tokenized_corpus[:1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ucoEJtOa2-7K",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "nb_samples = sum(len(s) for s in tokenized_corpus)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bfR6qIZZhIHd",
        "outputId": "ab099914-3bc9-45da-d1ef-0bbd619e7bb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 corpus items are [[12, 447, 202, 35, 41, 20, 12, 571, 11, 282, 2, 9, 8, 196, 1, 1549, 8, 201, 71, 123, 654, 319, 4500, 43, 2394, 58, 1408, 1478, 50, 483, 8, 196, 2, 50, 28, 572, 664, 20, 1, 3444, 458, 616, 450, 9, 388, 38, 1, 27, 4501, 53, 178, 664, 25, 1, 1631, 15, 46, 41, 1, 138, 85, 600, 4, 1632, 2, 46, 43, 2217, 2726, 9, 8, 1388, 2, 693, 1, 66, 74, 109, 23, 86, 178, 163, 17, 77, 356, 632, 45, 43, 1036, 2, 2395, 80, 130, 54, 15, 113, 9, 9, 8, 99, 170, 140, 20, 1, 122, 545, 196, 3, 23, 1, 475, 2218, 3230, 770, 1409, 2727, 2, 9, 8, 301, 2, 108, 9, 154, 16, 144, 859, 6, 43, 8190, 243, 16, 8, 99, 2, 9, 364, 123, 1, 179, 998, 9, 8, 1, 66, 812, 74, 109, 23, 750, 3, 142, 139, 5, 48, 64], [3, 19, 69, 730, 273, 62, 107, 187, 197, 351, 52, 14, 27, 9, 731, 5, 610, 15, 15, 59, 551, 272, 17, 22, 305, 8191, 52, 190, 13, 43, 335, 1821, 33, 22, 157, 107, 38, 13, 10, 121, 928, 12, 283, 2, 3, 572, 26, 52, 159, 560, 1367, 14, 524, 633, 9, 8, 178, 897, 73, 97, 3, 320, 11, 4, 633, 715, 2, 320, 18, 67, 19, 5, 139, 1317, 5, 46, 4, 915, 21, 17, 171, 1550, 30, 634, 54, 1, 642, 382, 64, 45, 578, 371, 292, 18, 32, 634, 26, 159, 5688, 2, 1, 512, 154, 2, 94, 50, 239, 898, 272, 8, 44, 826, 45, 1, 1440, 13, 634, 83, 5, 1, 512, 5, 1, 285, 1, 204, 32, 44, 34, 29, 82, 18, 1593, 50, 898, 254, 18, 324, 61, 18, 295, 26, 417, 8192, 18, 1018, 1, 884, 2872, 2873, 2, 1, 245, 1410, 1, 330, 148, 25, 18, 59, 200, 103, 80, 1, 2873, 8, 257, 2, 18, 94, 1, 5044, 42, 885, 2, 94, 1, 245, 840, 148, 200, 32, 174, 12, 283, 672, 1, 148, 138, 2, 3, 672, 1, 2873, 138, 1, 2873, 325, 19, 4, 8193, 180, 21, 253, 101, 3, 38, 12, 180, 18, 23, 5, 860, 352, 6, 1, 148, 5, 129, 9, 165, 2, 18, 32, 39, 1, 361, 68, 417, 5689, 25, 140, 8, 31, 2, 24, 38, 217, 197, 1979, 13, 731, 5, 610, 15, 13, 15, 19, 5, 103, 217, 218, 600, 88, 36, 217, 197, 1979, 19, 62, 1504, 1368], [11066, 11067, 2, 3, 60, 11068, 410, 595, 51, 44, 1677, 2, 5045, 68, 4, 222, 6, 4502, 4, 622, 11069, 2396, 3024, 11070, 2, 4, 2054, 16, 4503, 1, 266, 595, 2, 8194, 4085, 325, 4, 259, 418, 6, 2219, 1, 595, 232, 2, 6697, 15, 59, 166, 2220, 4504, 2, 11071, 540, 53, 11072, 799, 36, 100, 1, 595, 2, 4502, 1, 5690, 10, 194, 7, 257, 5, 296, 1, 740, 513, 751, 2, 11073], [1551, 441, 2494, 11074, 7, 4, 34, 623, 24, 5, 48, 182, 1713, 21, 296, 35, 4086, 15, 37, 15, 19, 121, 1368, 6698, 1099, 999, 16, 2494, 2, 732, 1, 357, 16, 62, 2601, 29, 15, 1287, 55, 928, 2, 344, 30, 655, 37, 15, 79, 411, 39, 546, 1633, 29, 3, 40, 119, 38, 3, 75, 124, 5691, 22, 8195, 51, 101, 18, 6699, 45, 80, 13, 7, 1182, 494, 5, 2494, 2, 158, 174, 93, 818, 94, 4, 284, 11, 517], [1479, 61, 212, 527, 2, 368, 41, 87, 3, 611, 41, 3, 23, 5, 48, 64, 1, 177, 134, 11, 73, 1, 28, 7, 13, 34, 14, 649, 95, 390, 1183, 385, 19, 656, 11075, 11076, 37, 3, 1714, 56, 1079, 153, 4505, 3231, 5, 1164, 3025, 170, 448, 5, 1080, 1764, 6700, 488, 1, 1871, 1411, 733, 1765, 12, 1146, 2, 12, 95, 4506, 2728, 11077, 11, 2602, 4, 131, 27, 5, 103, 9, 388, 11078, 45, 1, 319, 21, 54, 3, 694, 1, 361, 3, 8, 345, 26, 531, 68, 1, 514, 1081, 2, 3026, 321, 3, 126, 185, 11, 136, 5, 48, 1, 122, 8, 174, 3, 288, 1120, 36, 1, 555, 4507, 1262, 3729, 1822, 1822, 977, 190, 1412, 10, 1019, 1594, 9, 123, 9, 1288, 5, 567, 190, 1410, 61, 74, 23, 25, 260, 791, 5046, 424, 4508, 2, 4509, 5047, 5692, 11079, 650, 17, 22, 200, 32, 44, 108, 800, 21, 1, 424, 4508, 5048, 1, 610, 25, 110, 303, 3, 5693, 62, 1823, 45, 12, 11080, 3729, 2, 1822, 1822, 5049, 2603, 293, 4, 1766, 1, 495, 141, 7, 2604, 3, 65, 2605, 53, 3, 8, 822, 5, 103, 1, 1480, 495, 21, 9, 8, 84, 215, 10, 469, 9, 36, 8, 21, 86, 4, 206, 6701, 54, 9, 382, 5, 215, 1262, 1, 2495, 7, 5694, 2, 108, 17, 1505, 3445, 2, 62, 916, 10, 33, 84, 71, 7, 4, 498, 1263, 29, 37, 1, 34, 28, 298, 216, 5, 2055, 35, 100, 1, 801, 10, 14, 92, 11081, 119, 86, 4, 3232, 11, 331, 5050, 801, 2, 8196, 8197, 7, 12, 6702, 485, 4, 4087, 6, 106, 2, 2874, 1481, 100, 1, 495, 141, 51, 99, 36, 1, 4088, 22, 31, 60, 1, 2729]]\n",
            "Length of corpus is 4056\n"
          ]
        }
      ],
      "source": [
        "print(f'First 5 corpus items are {tokenized_corpus[:5]}')\n",
        "print(f'Length of corpus is {len(tokenized_corpus)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2B_z5Udki-_s",
        "outputId": "9e1baedd-8027-47ff-8ab8-9cdd1f670e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "type(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "B_Z1eJZrhK7K"
      },
      "outputs": [],
      "source": [
        "# This is the algorithmic part of batching the dataset and yielding the window of words and expected middle word for each bacth as a generator.\n",
        "def generate_data(corpus, vocab_size, window_size=2, sentence_batch_size=15,  batch_size=250):\n",
        "    number_of_sentence_batches = (len(corpus) // sentence_batch_size) + 1\n",
        "    for batch in range(number_of_sentence_batches):\n",
        "        lower_end = batch*batch_size\n",
        "        upper_end = (batch+1)*batch_size if batch+1 < number_of_sentence_batches else len(corpus)\n",
        "        mini_batch_size = upper_end - lower_end\n",
        "        maxlen = window_size*2\n",
        "        X = []\n",
        "        Y = []\n",
        "        for review_id, words in enumerate(corpus[lower_end:upper_end]):\n",
        "            L = len(words)\n",
        "            for index, word in enumerate(words):\n",
        "                contexts = []\n",
        "                labels   = []\n",
        "                s = index - window_size\n",
        "                e = index + window_size + 1\n",
        "\n",
        "                contexts.append([words[i] for i in range(s, e) if 0 <= i < L and i != index])\n",
        "                labels.append(word)\n",
        "\n",
        "                x = pad_sequences(contexts, maxlen=maxlen)\n",
        "                y = to_categorical(labels, vocab_size)\n",
        "                X.append(x)\n",
        "                Y.append(y)\n",
        "        X = tf.constant(X)\n",
        "        Y = tf.constant(Y)\n",
        "        number_of_batches = len(X) // batch_size\n",
        "        for real_batch in range(number_of_batches):\n",
        "          lower_end = real_batch*batch_size\n",
        "          upper_end = (real_batch+1)*batch_size\n",
        "          batch_X = tf.squeeze(X[lower_end:upper_end])\n",
        "          batch_Y = tf.squeeze(Y[lower_end:upper_end])\n",
        "          yield (batch_X, batch_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "nfsYbRRS2-7N",
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Notice now in a sample how we construct X and y to predict words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OvOclN8T2-7N",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "iterable = generate_data(corpus=tokenized_corpus[:10], vocab_size=vocab_size, batch_size=64)\n",
        "sample_x, sample_y = next(iterable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ak0gTIQs2-7O",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "43c1c1ae-6a51-44d3-ef20-c0926a493512",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 4), dtype=int32, numpy=\n",
              "array([[   0,    0,  447,  202],\n",
              "       [   0,   12,  202,   35],\n",
              "       [  12,  447,   35,   41],\n",
              "       [ 447,  202,   41,   20],\n",
              "       [ 202,   35,   20,   12],\n",
              "       [  35,   41,   12,  571],\n",
              "       [  41,   20,  571,   11],\n",
              "       [  20,   12,   11,  282],\n",
              "       [  12,  571,  282,    2],\n",
              "       [ 571,   11,    2,    9],\n",
              "       [  11,  282,    9,    8],\n",
              "       [ 282,    2,    8,  196],\n",
              "       [   2,    9,  196,    1],\n",
              "       [   9,    8,    1, 1549],\n",
              "       [   8,  196, 1549,    8],\n",
              "       [ 196,    1,    8,  201],\n",
              "       [   1, 1549,  201,   71],\n",
              "       [1549,    8,   71,  123],\n",
              "       [   8,  201,  123,  654],\n",
              "       [ 201,   71,  654,  319],\n",
              "       [  71,  123,  319, 4500],\n",
              "       [ 123,  654, 4500,   43],\n",
              "       [ 654,  319,   43, 2394],\n",
              "       [ 319, 4500, 2394,   58],\n",
              "       [4500,   43,   58, 1408],\n",
              "       [  43, 2394, 1408, 1478],\n",
              "       [2394,   58, 1478,   50],\n",
              "       [  58, 1408,   50,  483],\n",
              "       [1408, 1478,  483,    8],\n",
              "       [1478,   50,    8,  196],\n",
              "       [  50,  483,  196,    2],\n",
              "       [ 483,    8,    2,   50],\n",
              "       [   8,  196,   50,   28],\n",
              "       [ 196,    2,   28,  572],\n",
              "       [   2,   50,  572,  664],\n",
              "       [  50,   28,  664,   20],\n",
              "       [  28,  572,   20,    1],\n",
              "       [ 572,  664,    1, 3444],\n",
              "       [ 664,   20, 3444,  458],\n",
              "       [  20,    1,  458,  616],\n",
              "       [   1, 3444,  616,  450],\n",
              "       [3444,  458,  450,    9],\n",
              "       [ 458,  616,    9,  388],\n",
              "       [ 616,  450,  388,   38],\n",
              "       [ 450,    9,   38,    1],\n",
              "       [   9,  388,    1,   27],\n",
              "       [ 388,   38,   27, 4501],\n",
              "       [  38,    1, 4501,   53],\n",
              "       [   1,   27,   53,  178],\n",
              "       [  27, 4501,  178,  664],\n",
              "       [4501,   53,  664,   25],\n",
              "       [  53,  178,   25,    1],\n",
              "       [ 178,  664,    1, 1631],\n",
              "       [ 664,   25, 1631,   15],\n",
              "       [  25,    1,   15,   46],\n",
              "       [   1, 1631,   46,   41],\n",
              "       [1631,   15,   41,    1],\n",
              "       [  15,   46,    1,  138],\n",
              "       [  46,   41,  138,   85],\n",
              "       [  41,    1,   85,  600],\n",
              "       [   1,  138,  600,    4],\n",
              "       [ 138,   85,    4, 1632],\n",
              "       [  85,  600, 1632,    2],\n",
              "       [ 600,    4,    2,   46]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "sample_y_numpy = sample_y.numpy()\n",
        "\n",
        "sample_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ix8s4Knh2-7O",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "77230348-2793-4212-d877-67c0e8015253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]),\n",
              " array([  12,  447,  202,   35,   41,   20,   12,  571,   11,  282,    2,\n",
              "           9,    8,  196,    1, 1549,    8,  201,   71,  123,  654,  319,\n",
              "        4500,   43, 2394,   58, 1408, 1478,   50,  483,    8,  196,    2,\n",
              "          50,   28,  572,  664,   20,    1, 3444,  458,  616,  450,    9,\n",
              "         388,   38,    1,   27, 4501,   53,  178,  664,   25,    1, 1631,\n",
              "          15,   46,   41,    1,  138,   85,  600,    4, 1632]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "np.where(sample_y_numpy == 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpvnqGOI2-7O",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now comes the core part, defining the model. Keras provides a convenient Sequential model class to just `add` layers of any type and they will just work. Let's add an `Embedding` layer (that will map the word ids into a vector of size 100), a `Lambda` to average the words out in a sentence, and a `Dense layer` to select the best word on the other end. This is classic CBOW.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CHtu75Kpi6XF"
      },
      "outputs": [],
      "source": [
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=4))  # Add an Embedding layer with input_dim vocab_size, output_dim to be embedding_dim, and the input_length to be twice our window\n",
        "cbow.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))  # Add a Lambda that takes a lambda function using the K.mean method to average the words. The output_shape should be (dim, ).\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))  # Add a classic Dense layer to just select with a softmax the best word\n",
        "# Compile the model with a loss and optimizer of your liking.\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Yt2xo_G02-7O",
        "pycharm": {
          "name": "#%%\n"
        },
        "outputId": "a77cc958-2ddf-4d44-ee9e-07787990f55e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 50)             997500    \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 50)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 19950)             1017450   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2014950 (7.69 MB)\n",
            "Trainable params: 2014950 (7.69 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "cbow.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_y.shape"
      ],
      "metadata": {
        "id": "pXwC8tlnKklp",
        "outputId": "8ce0b11d-8886-4768-fd5d-7357a1aa5698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 19950])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g44ICdUcj7ZL"
      },
      "outputs": [],
      "source": [
        "def fit_model():\n",
        "    if not BATCH:\n",
        "        # If we are not batching, Fill how to get X AND Y\n",
        "        X, Y = None # Fill\n",
        "        print(f'Size of X is {X.shape} and Y is {Y.shape}')\n",
        "        cbow.fit(X, Y, epochs = epochs)\n",
        "    else:\n",
        "        for epoch in range(epochs):\n",
        "          batch_count = 0\n",
        "          for x, y in generate_data(corpus=tokenized_corpus[:100], vocab_size=vocab_size, batch_size=batch_size):\n",
        "              print(f\"Epoch {epoch+1} Batch {batch_count+1}\")\n",
        "              history = cbow.train_on_batch(x, y, return_dict=True)\n",
        "              print(f'Loss: {history[\"loss\"]}')\n",
        "              batch_count += 1\n",
        "              if batch_count > batch_size:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qTM2wqbzke5n",
        "outputId": "137b635f-0800-4584-d426-e32dd2a8b431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 1\n",
            "Loss: 9.900993347167969\n",
            "Epoch 1 Batch 2\n",
            "Loss: 9.900312423706055\n",
            "Epoch 1 Batch 3\n",
            "Loss: 9.89998722076416\n",
            "Epoch 1 Batch 4\n",
            "Loss: 9.89941692352295\n",
            "Epoch 1 Batch 5\n",
            "Loss: 9.898660659790039\n",
            "Epoch 1 Batch 6\n",
            "Loss: 9.89686393737793\n",
            "Epoch 1 Batch 7\n",
            "Loss: 9.897046089172363\n",
            "Epoch 1 Batch 8\n",
            "Loss: 9.897134780883789\n",
            "Epoch 1 Batch 9\n",
            "Loss: 9.895523071289062\n",
            "Epoch 1 Batch 10\n",
            "Loss: 9.895886421203613\n",
            "Epoch 1 Batch 11\n",
            "Loss: 9.892911911010742\n",
            "Epoch 1 Batch 12\n",
            "Loss: 9.893547058105469\n",
            "Epoch 1 Batch 13\n",
            "Loss: 9.890342712402344\n",
            "Epoch 1 Batch 14\n",
            "Loss: 9.891571044921875\n",
            "Epoch 1 Batch 15\n",
            "Loss: 9.892364501953125\n",
            "Epoch 1 Batch 16\n",
            "Loss: 9.889825820922852\n",
            "Epoch 1 Batch 17\n",
            "Loss: 9.885358810424805\n",
            "Epoch 1 Batch 18\n",
            "Loss: 9.883279800415039\n",
            "Epoch 1 Batch 19\n",
            "Loss: 9.884292602539062\n",
            "Epoch 1 Batch 20\n",
            "Loss: 9.887998580932617\n",
            "Epoch 1 Batch 21\n",
            "Loss: 9.888845443725586\n",
            "Epoch 1 Batch 22\n",
            "Loss: 9.883516311645508\n",
            "Epoch 1 Batch 23\n",
            "Loss: 9.880757331848145\n",
            "Epoch 1 Batch 24\n",
            "Loss: 9.879609107971191\n",
            "Epoch 1 Batch 25\n",
            "Loss: 9.879585266113281\n",
            "Epoch 1 Batch 26\n",
            "Loss: 9.878376007080078\n",
            "Epoch 1 Batch 27\n",
            "Loss: 9.877260208129883\n",
            "Epoch 1 Batch 28\n",
            "Loss: 9.875285148620605\n",
            "Epoch 1 Batch 29\n",
            "Loss: 9.870984077453613\n",
            "Epoch 1 Batch 30\n",
            "Loss: 9.876703262329102\n",
            "Epoch 1 Batch 31\n",
            "Loss: 9.870208740234375\n",
            "Epoch 1 Batch 32\n",
            "Loss: 9.869937896728516\n",
            "Epoch 1 Batch 33\n",
            "Loss: 9.872255325317383\n",
            "Epoch 1 Batch 34\n",
            "Loss: 9.871496200561523\n",
            "Epoch 1 Batch 35\n",
            "Loss: 9.875184059143066\n",
            "Epoch 1 Batch 36\n",
            "Loss: 9.87246036529541\n",
            "Epoch 1 Batch 37\n",
            "Loss: 9.86569595336914\n",
            "Epoch 1 Batch 38\n",
            "Loss: 9.871820449829102\n",
            "Epoch 1 Batch 39\n",
            "Loss: 9.859722137451172\n",
            "Epoch 1 Batch 40\n",
            "Loss: 9.864246368408203\n",
            "Epoch 1 Batch 41\n",
            "Loss: 9.861258506774902\n",
            "Epoch 1 Batch 42\n",
            "Loss: 9.856369018554688\n",
            "Epoch 1 Batch 43\n",
            "Loss: 9.86689281463623\n",
            "Epoch 1 Batch 44\n",
            "Loss: 9.860240936279297\n",
            "Epoch 1 Batch 45\n",
            "Loss: 9.8547945022583\n",
            "Epoch 1 Batch 46\n",
            "Loss: 9.857832908630371\n",
            "Epoch 1 Batch 47\n",
            "Loss: 9.852394104003906\n",
            "Epoch 1 Batch 48\n",
            "Loss: 9.854732513427734\n",
            "Epoch 1 Batch 49\n",
            "Loss: 9.852983474731445\n",
            "Epoch 1 Batch 50\n",
            "Loss: 9.854562759399414\n",
            "Epoch 1 Batch 51\n",
            "Loss: 9.849576950073242\n",
            "Epoch 1 Batch 52\n",
            "Loss: 9.844193458557129\n",
            "Epoch 1 Batch 53\n",
            "Loss: 9.83553409576416\n",
            "Epoch 1 Batch 54\n",
            "Loss: 9.845407485961914\n",
            "Epoch 1 Batch 55\n",
            "Loss: 9.851094245910645\n",
            "Epoch 1 Batch 56\n",
            "Loss: 9.828018188476562\n",
            "Epoch 1 Batch 57\n",
            "Loss: 9.853313446044922\n",
            "Epoch 1 Batch 58\n",
            "Loss: 9.84581184387207\n",
            "Epoch 1 Batch 59\n",
            "Loss: 9.846385955810547\n",
            "Epoch 1 Batch 60\n",
            "Loss: 9.84868049621582\n",
            "Epoch 1 Batch 61\n",
            "Loss: 9.840771675109863\n",
            "Epoch 1 Batch 62\n",
            "Loss: 9.839014053344727\n",
            "Epoch 1 Batch 63\n",
            "Loss: 9.83513069152832\n",
            "Epoch 1 Batch 64\n",
            "Loss: 9.83001708984375\n",
            "Epoch 1 Batch 65\n",
            "Loss: 9.826438903808594\n",
            "Epoch 2 Batch 1\n",
            "Loss: 9.79074478149414\n",
            "Epoch 2 Batch 2\n",
            "Loss: 9.786859512329102\n",
            "Epoch 2 Batch 3\n",
            "Loss: 9.784761428833008\n",
            "Epoch 2 Batch 4\n",
            "Loss: 9.785665512084961\n",
            "Epoch 2 Batch 5\n",
            "Loss: 9.78472900390625\n",
            "Epoch 2 Batch 6\n",
            "Loss: 9.772703170776367\n",
            "Epoch 2 Batch 7\n",
            "Loss: 9.781229019165039\n",
            "Epoch 2 Batch 8\n",
            "Loss: 9.782852172851562\n",
            "Epoch 2 Batch 9\n",
            "Loss: 9.776956558227539\n",
            "Epoch 2 Batch 10\n",
            "Loss: 9.777355194091797\n",
            "Epoch 2 Batch 11\n",
            "Loss: 9.7645902633667\n",
            "Epoch 2 Batch 12\n",
            "Loss: 9.781578063964844\n",
            "Epoch 2 Batch 13\n",
            "Loss: 9.751494407653809\n",
            "Epoch 2 Batch 14\n",
            "Loss: 9.764554977416992\n",
            "Epoch 2 Batch 15\n",
            "Loss: 9.773577690124512\n",
            "Epoch 2 Batch 16\n",
            "Loss: 9.758496284484863\n",
            "Epoch 2 Batch 17\n",
            "Loss: 9.734769821166992\n",
            "Epoch 2 Batch 18\n",
            "Loss: 9.711762428283691\n",
            "Epoch 2 Batch 19\n",
            "Loss: 9.716923713684082\n",
            "Epoch 2 Batch 20\n",
            "Loss: 9.75908374786377\n",
            "Epoch 2 Batch 21\n",
            "Loss: 9.762155532836914\n",
            "Epoch 2 Batch 22\n",
            "Loss: 9.729523658752441\n",
            "Epoch 2 Batch 23\n",
            "Loss: 9.718107223510742\n",
            "Epoch 2 Batch 24\n",
            "Loss: 9.705137252807617\n",
            "Epoch 2 Batch 25\n",
            "Loss: 9.708904266357422\n",
            "Epoch 2 Batch 26\n",
            "Loss: 9.709789276123047\n",
            "Epoch 2 Batch 27\n",
            "Loss: 9.695943832397461\n",
            "Epoch 2 Batch 28\n",
            "Loss: 9.696054458618164\n",
            "Epoch 2 Batch 29\n",
            "Loss: 9.666357040405273\n",
            "Epoch 2 Batch 30\n",
            "Loss: 9.699629783630371\n",
            "Epoch 2 Batch 31\n",
            "Loss: 9.664509773254395\n",
            "Epoch 2 Batch 32\n",
            "Loss: 9.658723831176758\n",
            "Epoch 2 Batch 33\n",
            "Loss: 9.689423561096191\n",
            "Epoch 2 Batch 34\n",
            "Loss: 9.672614097595215\n",
            "Epoch 2 Batch 35\n",
            "Loss: 9.705451965332031\n",
            "Epoch 2 Batch 36\n",
            "Loss: 9.68204116821289\n",
            "Epoch 2 Batch 37\n",
            "Loss: 9.63896369934082\n",
            "Epoch 2 Batch 38\n",
            "Loss: 9.664133071899414\n",
            "Epoch 2 Batch 39\n",
            "Loss: 9.611637115478516\n",
            "Epoch 2 Batch 40\n",
            "Loss: 9.638724327087402\n",
            "Epoch 2 Batch 41\n",
            "Loss: 9.611026763916016\n",
            "Epoch 2 Batch 42\n",
            "Loss: 9.591329574584961\n",
            "Epoch 2 Batch 43\n",
            "Loss: 9.639352798461914\n",
            "Epoch 2 Batch 44\n",
            "Loss: 9.619039535522461\n",
            "Epoch 2 Batch 45\n",
            "Loss: 9.576520919799805\n",
            "Epoch 2 Batch 46\n",
            "Loss: 9.599895477294922\n",
            "Epoch 2 Batch 47\n",
            "Loss: 9.565816879272461\n",
            "Epoch 2 Batch 48\n",
            "Loss: 9.571144104003906\n",
            "Epoch 2 Batch 49\n",
            "Loss: 9.561984062194824\n",
            "Epoch 2 Batch 50\n",
            "Loss: 9.586337089538574\n",
            "Epoch 2 Batch 51\n",
            "Loss: 9.546607971191406\n",
            "Epoch 2 Batch 52\n",
            "Loss: 9.519384384155273\n",
            "Epoch 2 Batch 53\n",
            "Loss: 9.476116180419922\n",
            "Epoch 2 Batch 54\n",
            "Loss: 9.536853790283203\n",
            "Epoch 2 Batch 55\n",
            "Loss: 9.572212219238281\n",
            "Epoch 2 Batch 56\n",
            "Loss: 9.427079200744629\n",
            "Epoch 2 Batch 57\n",
            "Loss: 9.57419204711914\n",
            "Epoch 2 Batch 58\n",
            "Loss: 9.526338577270508\n",
            "Epoch 2 Batch 59\n",
            "Loss: 9.53034782409668\n",
            "Epoch 2 Batch 60\n",
            "Loss: 9.529569625854492\n",
            "Epoch 2 Batch 61\n",
            "Loss: 9.516438484191895\n",
            "Epoch 2 Batch 62\n",
            "Loss: 9.499488830566406\n",
            "Epoch 2 Batch 63\n",
            "Loss: 9.494195938110352\n",
            "Epoch 2 Batch 64\n",
            "Loss: 9.44883918762207\n",
            "Epoch 2 Batch 65\n",
            "Loss: 9.423606872558594\n"
          ]
        }
      ],
      "source": [
        "fit_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GR97HVOqkoMI"
      },
      "outputs": [],
      "source": [
        "with open('./cbow_scratch_synonims.txt' ,'w') as f:\n",
        "    f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
        "    vectors = cbow.get_weights()[0]\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        str_vec = ' '.join(map(str, list(vectors[i, :])))\n",
        "        f.write('{} {}\\n'.format(word, str_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SvMp9eWsk2Z-"
      },
      "outputs": [],
      "source": [
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./cbow_scratch_synonims.txt', binary=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J0dw_S7Kk6lW",
        "outputId": "9ba6745f-39d4-471f-eaf4-acda869ef86f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"'yellow\", 0.5359535813331604),\n",
              " ('louis', 0.5183405876159668),\n",
              " ('ridgemont', 0.48729079961776733),\n",
              " ('harmony', 0.47583910822868347),\n",
              " ('smiling', 0.4705735743045807),\n",
              " ('seeded', 0.4645508825778961),\n",
              " ('mormonic', 0.4609116017818451),\n",
              " ('goodness', 0.4587244689464569),\n",
              " ('menu', 0.4559972584247589),\n",
              " ('piercings', 0.451995849609375)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "w2v.most_similar(positive=['gasoline'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "JCmSyCj8k6He",
        "outputId": "f27df369-9882-4b4b-feae-7073c0ee3e74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('artistic', 0.5652351379394531),\n",
              " ('teacher', 0.5102579593658447),\n",
              " ('song', 0.4745451509952545),\n",
              " ('ficticious', 0.46586015820503235),\n",
              " ('embarrassment', 0.462168425321579),\n",
              " ('wonderous', 0.4610004723072052),\n",
              " ('flatbreads', 0.45966923236846924),\n",
              " ('hurried', 0.4594070315361023),\n",
              " ('marble', 0.456620991230011),\n",
              " ('found', 0.4562104046344757)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "w2v.most_similar(negative=['apple'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wjvrCdY5lkJk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}